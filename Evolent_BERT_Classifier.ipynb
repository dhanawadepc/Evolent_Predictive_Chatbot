{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "XUB3ITsrn2U0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "from transformers import AutoModel, BertTokenizerFast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EltIzfuQoMdX"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import warnings\n",
    "import string\n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_colwidth',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KzPptGVviHCO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-17 12:16:52.338995: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import torch, os\n",
    "import pandas as pd\n",
    "from transformers import pipeline, BertForSequenceClassification, BertTokenizerFast\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "JxF0NsU4iKh1",
    "outputId": "3252d024-ec59-4999-e182-395ef83dfcaf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import cuda\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "qMyU9jkdoO4-"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/pradeepdhanawade/Pradeep_Personal/Evolent/train.csv',sep='\\t',names=['class','text'],header=None)\n",
    "# data.head(2)\n",
    "data = data[:10000]\n",
    "# data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {1:0,2:1,3:2,4:3,5:4}\n",
    "data['class'] = data['class'].map(class_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JOe3F_LsrJKr",
    "outputId": "761eb54a-a012-4d02-d759-9aaf50796ba2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 1, 0, 2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "B6uCs_P0ozur"
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('/home/pradeepdhanawade/Pradeep_Personal/Evolent/test.csv',sep='\\t',names=['text'],header=None)\n",
    "data_test.head(2)\n",
    "data_test = data_test[:100]\n",
    "# data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "Wbw2MfqQoO7R"
   },
   "outputs": [],
   "source": [
    "x = data['text']\n",
    "y = data['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "BflEHqIzrYIW"
   },
   "outputs": [],
   "source": [
    "y = y.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "TIO5bwy9xavI"
   },
   "outputs": [],
   "source": [
    "x_test = data_test['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "89BdzUheoO9j"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# import BERT-base pretrained model\n",
    "bert = AutoModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "PjERwV_ooO_3"
   },
   "outputs": [],
   "source": [
    "tokens_train = tokenizer.batch_encode_plus(x.to_list(),padding=True,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "rCh4AL_AoPCD"
   },
   "outputs": [],
   "source": [
    "tokens_test = tokenizer.batch_encode_plus(x_test.to_list(), padding=True,truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "Rp9p03SjoPEP"
   },
   "outputs": [],
   "source": [
    "train_sequence = torch.tensor(tokens_train['input_ids'])\n",
    "train_mask_sequence = torch.tensor(tokens_train['attention_mask'])\n",
    "train_label_sequence = torch.tensor(y.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "s2KlDD4CoPGo"
   },
   "outputs": [],
   "source": [
    "test_sequence = torch.tensor(tokens_test['input_ids'])\n",
    "test_mask_sequence = torch.tensor(tokens_test['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "rWuh1UXXoPJd"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset,DataLoader,RandomSampler,SequentialSampler,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "xr9ckIgQpbb4"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "# wrap tensors\n",
    "train_data = TensorDataset(train_sequence,train_mask_sequence,train_label_sequence)\n",
    "# sampler for sampling the data during training\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data,sampler=train_sampler,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9slaOkUpw99"
   },
   "source": [
    "# **Model** Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "fiCjKJNVpcX9"
   },
   "outputs": [],
   "source": [
    "# freeze all the parameters\n",
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "IDPpG9nUpcad"
   },
   "outputs": [],
   "source": [
    "class BERT_Arch(nn.Module):\n",
    "\n",
    "    def __init__(self, bert):\n",
    "        super(BERT_Arch, self).__init__()\n",
    "\n",
    "        self.bert = bert\n",
    "\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(768,512)\n",
    "\n",
    "        self.fc2 = nn.Linear(512,5)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, sent_id, mask):\n",
    "\n",
    "        #pass the inputs to the model\n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "\n",
    "        x = self.fc1(cls_hs)\n",
    "\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "DOmmVVZppccw"
   },
   "outputs": [],
   "source": [
    "# pass the pre-trained BERT to our define architecture\n",
    "model = BERT_Arch(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "mnBUnp3U0QKX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BERT_Arch(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (relu): ReLU()\n",
       "  (fc1): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model = model.to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "vSmt7KSspcfF"
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(),lr = 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H8KVg0whtSuR",
    "outputId": "e46ef7a5-6c6f-4fd2-9151-b6486bfaa85c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecrzzuzspchy",
    "outputId": "423868ae-c6e7-4016-ace1-8e54777b26f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights: [0.90826521 1.89753321 1.48809524 0.93940817 0.61143381]\n",
      "type of cross entropy: <class 'torch.nn.modules.loss.CrossEntropyLoss'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "#compute the class weights\n",
    "# class_weights = compute_class_weight(class_weight='balanced',classes= np.unique(y),y= y)\n",
    "class_weights = compute_class_weight(class_weight='balanced',classes= np.unique(y),y= y)\n",
    "print('Class Weights:',class_weights)\n",
    "\n",
    "# converting list of class weights to a tensor\n",
    "weights= torch.tensor(class_weights,dtype=torch.float)\n",
    "# print('weights: ',weights)\n",
    "# push to GPU\n",
    "# weights = weights.to(device)\n",
    "\n",
    "# define the loss function\n",
    "# cross_entropy  = nn.NLLLoss(weight=weights)\n",
    "cross_entropy = nn.CrossEntropyLoss(weight=weights)\n",
    "print('type of cross entropy:', type(cross_entropy))\n",
    "# print('entropy value:', cross_entropy)\n",
    "# number of training epochs\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcrgXMPVqitI"
   },
   "source": [
    "# Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "PJI9Ytp2pclV"
   },
   "outputs": [],
   "source": [
    "# function to train the model\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "\n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "\n",
    "        # progress update after every 50 batches.\n",
    "        if step % 50 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "\n",
    "        # push the batch to gpu\n",
    "#         batch = [r.to(device) for r in batch]\n",
    "\n",
    "        sent_id, mask, labels = batch\n",
    "#         print('sent_id: ', sent_id)\n",
    "#         print('mask: ', mask)\n",
    "#         print('labels: ', labels)\n",
    "\n",
    "        # clear previously calculated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get model predictions for the current batch\n",
    "        preds = model(sent_id, mask)\n",
    "#         print('preds: ', preds)\n",
    "#         print('pred size: ',preds.shape)\n",
    "#         print('label size: ', labels.shape)\n",
    "        \n",
    "        # compute the loss between actual and predicted values\n",
    "        loss = cross_entropy(preds, labels)\n",
    "        print('loss: ',loss)\n",
    "#         break\n",
    "        # add on to the total loss\n",
    "        total_loss = total_loss + loss.item()\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the the gradients to 1.0. It helps in preventing the exploding gradient problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "    # append the model predictions\n",
    "    total_preds.append(preds)\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "      # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "      # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss, total_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vPoiK8s2pEYo"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 440
    },
    "id": "zAS2_znlpcoH",
    "outputId": "ca69d32d-f604-4f13-8426-9255e5610b09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoch 1 / 1\n",
      "loss:  tensor(1.5952, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5678, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5904, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6025, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6500, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5880, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6399, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5898, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6331, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6165, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5757, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6449, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5900, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6091, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6484, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6017, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5855, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6405, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6088, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6355, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5910, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6295, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6389, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6081, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5955, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6134, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6405, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6353, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6135, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6358, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5785, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5975, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6102, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5717, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6232, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5896, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6226, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6088, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6218, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5894, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6280, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6099, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6059, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5892, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5982, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6174, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6269, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6082, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6097, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5803, grad_fn=<NllLossBackward0>)\n",
      "  Batch    50  of    313.\n",
      "loss:  tensor(1.6183, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6307, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6147, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6440, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5861, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6078, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5928, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6020, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6319, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6037, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6121, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6010, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6191, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6148, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6096, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6069, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5931, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6191, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6149, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6265, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6243, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6055, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6339, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6115, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6264, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5969, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5898, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5907, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6031, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6242, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6112, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6217, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5915, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6193, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6312, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6127, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6079, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6333, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5923, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5967, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5984, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5896, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6148, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6123, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5705, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6046, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6189, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6150, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6329, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6094, grad_fn=<NllLossBackward0>)\n",
      "  Batch   100  of    313.\n",
      "loss:  tensor(1.6096, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6128, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5988, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6119, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5821, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6370, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6213, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6099, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6045, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5785, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5874, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6196, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6086, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6076, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6372, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6215, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6116, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5901, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6112, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5981, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6207, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6106, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6247, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6114, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5950, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5868, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6040, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5941, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6185, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6189, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6116, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6274, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6248, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6124, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6236, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6235, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6101, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6067, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5879, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6289, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6524, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5857, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6005, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5975, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6163, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6115, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6039, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6115, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6142, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6140, grad_fn=<NllLossBackward0>)\n",
      "  Batch   150  of    313.\n",
      "loss:  tensor(1.6134, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6133, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6287, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6128, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6222, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6255, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6096, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6043, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5845, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6146, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6146, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6037, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6136, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(1.5971, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6093, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6219, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6017, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6114, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5963, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5958, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5967, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5966, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6132, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6052, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6030, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5940, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5998, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6106, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5980, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6108, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6147, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5748, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6018, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5985, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5841, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5968, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6026, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6082, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6076, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5946, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6191, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6095, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6071, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5865, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6143, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6069, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6052, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5810, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5963, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5973, grad_fn=<NllLossBackward0>)\n",
      "  Batch   200  of    313.\n",
      "loss:  tensor(1.6277, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5845, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6204, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6243, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5827, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6196, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5975, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5961, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6126, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6313, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6028, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5841, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6109, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5931, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6043, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6035, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6057, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6114, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5884, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5965, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5964, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5962, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5878, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5997, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5854, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5938, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5923, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5937, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6101, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6320, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6052, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6311, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6025, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5912, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6069, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6246, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5929, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6165, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6012, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5961, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6037, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6015, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6093, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6186, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6015, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5943, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5968, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5999, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6038, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5694, grad_fn=<NllLossBackward0>)\n",
      "  Batch   250  of    313.\n",
      "loss:  tensor(1.6376, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5946, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5882, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6079, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5964, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6175, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6103, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6117, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.6287, grad_fn=<NllLossBackward0>)\n",
      "loss:  tensor(1.5756, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# set initial loss to infinite\n",
    "best_valid_loss = float('inf')\n",
    "preds = model(test_sequence, test_mask_sequence)\n",
    "#defining epochs\n",
    "epochs = 1\n",
    "\n",
    "# empty lists to store training and validation loss of each epoch\n",
    "# train_losses=[]\n",
    "# valid_losses=[]\n",
    "\n",
    "#for each epoch\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
    "\n",
    "    #train model\n",
    "    train_loss, _ = train()\n",
    "\n",
    "    #evaluate model\n",
    "    # valid_loss, _ = evaluate()\n",
    "\n",
    "    #save the best model\n",
    "    # if valid_loss < best_valid_loss:\n",
    "    #     best_valid_loss = valid_loss\n",
    "    #     torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    torch.save(model.state_dict(), 'bertgenai.pt')\n",
    "    # append training and validation loss\n",
    "    # train_losses.append(train_loss)\n",
    "    # valid_losses.append(valid_loss)\n",
    "\n",
    "    # print(f'\\nTraining Loss: {train_loss:.3f}')\n",
    "    # print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Dj4pbMxSrX77"
   },
   "outputs": [],
   "source": [
    "# model.push_to_hub(\"aipradeepd/Evolent_BERT_Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
