{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0x3dXB6moQ_q"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.0.208 deeplake openai==0.27.8 tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dotenv"
      ],
      "metadata": {
        "id": "yZwfKu3uRloI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chainlit"
      ],
      "metadata": {
        "id": "7qhMlZjRELFz"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chainlit hello"
      ],
      "metadata": {
        "id": "03PecBWvESLm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "T5rByz1jo4Jd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i94FLyxzRLZh",
        "outputId": "a3d13299-4e37-4151-a99d-ef0ceca7598f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv('/content/drive/MyDrive/mykey.env')"
      ],
      "metadata": {
        "id": "ZJPwDmBCpKHf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af6b8ff9-15ab-4e4e-fd94-ec699c052ee2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install huggingface hub"
      ],
      "metadata": {
        "id": "5-3iWHTTSNlu"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Testing before creating chatbot in Chainlit\n",
        "\n",
        "# template = \"\"\"Medical Text: {abstract}\n",
        "\n",
        "# Class: \"\"\"\n",
        "# prompt = PromptTemplate(\n",
        "#     template=template,\n",
        "#     input_variables=['question']\n",
        "# )\n",
        "\n",
        "# # user question\n",
        "# question = \"Write a description here.\"\n",
        "\n",
        "# from langchain import HuggingFaceHub, LLMChain\n",
        "\n",
        "# # initialize Hub LLM\n",
        "# hub_llm = HuggingFaceHub(\n",
        "#         repo_id='aipradeepd/Evolent_BERT_Classifier',\n",
        "#     model_kwargs={'temperature':0}\n",
        "# )\n",
        "\n",
        "# # create prompt template > LLM chain\n",
        "# llm_chain = LLMChain(\n",
        "#     prompt=prompt,\n",
        "#     llm=hub_llm\n",
        "# )\n",
        "\n",
        "# # ask the user question about the capital of France\n",
        "# print(llm_chain.run(question))\n",
        "# # initialize Hub LLM\n",
        "# hub_llm = HuggingFaceHub(\n",
        "#         repo_id='aipradeepd/Evolent_BERT_Classifier',\n",
        "#     model_kwargs={'temperature':0}\n",
        "# )\n",
        "\n",
        "# # create prompt template > LLM chain\n",
        "# llm_chain = LLMChain(\n",
        "#     prompt=prompt,\n",
        "#     llm=hub_llm\n",
        "# )\n",
        "\n",
        "# # ask the user question about the capital of France\n",
        "# print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "XGn8DmixSNol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from chainlit import cl\n",
        "from langchain import HuggingFaceHub,PromptTemplate,LLMChain"
      ],
      "metadata": {
        "id": "SpXI6qxVSXKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id='aipradeepd/Evolent_BERT_Classifier'\n",
        "evolent_model = HuggingFaceHub(repo_id='aipradeepd/Evolent_BERT_Classifier',model_kwargs={'temperature':0})\n"
      ],
      "metadata": {
        "id": "RaMyUwITG6bk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\" You are a Medical Assistant that predicts the class of Medical test based on abstract received as a input\n",
        "\n",
        "\n",
        "{abstract}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "V8rla7HeSXMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@cl_on_chat_start\n",
        "def main():\n",
        "  prompt = PromptTemplate(template=template,input_variables=['abstract'])\n",
        "  model_chain = LLMChain(llm=evolent_model,prompt=prompt,verbose=True)\n",
        "\n",
        "  cl.user_session.set('evolent_chain',model_chain)\n"
      ],
      "metadata": {
        "id": "w6BiW-Z-SXOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@cl.on_message\n",
        "async def main(message:str):\n",
        "  evolent_chain = cl.user_session.get('evolent_chain')\n",
        "  res = await evolent_chain.acall(message,callbacks=[cl.AsyncLangchainCallbackHandler()])\n",
        "\n",
        "  await cl.Message(content=res['text']).send()\n",
        ""
      ],
      "metadata": {
        "id": "oLHOwiD8KBYj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_rmGYR7PKBay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "=============================================================="
      ],
      "metadata": {
        "id": "Xol8gsH7Rbno"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8EoLiWuzKBc6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}